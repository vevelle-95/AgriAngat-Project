# Updated requirements for Phi-4-mini-flash-reasoning with llama-cpp-python
llama-cpp-python[server]>=0.3.16
pandas>=2.0.0
chromadb>=1.0.0
sentence-transformers>=5.0.0
torch>=2.0.0
numpy>=1.20.0
huggingface-hub>=0.20.0
scikit-learn>=1.0.0
typing-extensions>=4.5.0
